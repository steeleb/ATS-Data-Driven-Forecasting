{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose \n",
    "\n",
    "This script creates a model for predicting water temperature in Shadow Mountain Reservoir using the t2023 data. This pass is a middle ground between the aggressive feature reduction and all features. For this model, we're using the `leaky_basic_5` setting in `settings.py`, which reduces the number of hidden layers.\n",
    "\n",
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#high level modules\n",
    "import os\n",
    "import imp\n",
    "import pandas as pd\n",
    "\n",
    "# ml/ai modules\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import custom modules\n",
    "this_dir = \"/Users/steeleb/Documents/GitHub/NASA-NW/modeling/temperature/arNN_fewer2/\"\n",
    "imp.load_source(\"settings\",os.path.join(this_dir,\"settings.py\"))\n",
    "from settings import settings\n",
    "imp.load_source(\"architecture\", os.path.join(this_dir, \"architecture.py\"))\n",
    "from architecture import build_model, compile_model\n",
    "imp.load_source(\"universals\", os.path.join(this_dir, \"universal_functions.py\"))\n",
    "from universals import save_to_pickle, twotemp_labels_features\n",
    "\n",
    "# point to data directory\n",
    "data_dir = \"/Users/steeleb/Documents/GitHub/NASA-NW/data/NN_train_val_test/SMR_autoNN_reduce_2/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import train/val sets\n",
    "\n",
    "Import and format training and validation arrays for use in model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = pd.Series(os.listdir(data_dir))\n",
    "t2023 = all_files[all_files.str.contains('t2023')]\n",
    "t2023_val = t2023[t2023.str.contains('validation')]\n",
    "t2023_train = t2023[t2023.str.contains('training')]\n",
    "\n",
    "# these files end up in no particular order, so we need to sort them\n",
    "t2023_val = t2023_val.sort_values()\n",
    "t2023_train = t2023_train.sort_values()\n",
    "\n",
    "def load_data(file):\n",
    "    return pd.read_csv(os.path.join(data_dir, file), sep=',')\n",
    "\n",
    "val1 = load_data(t2023_val.values[0])\n",
    "train1 = load_data(t2023_train.values[0])\n",
    "\n",
    "val2 = load_data(t2023_val.values[1])\n",
    "train2 = load_data(t2023_train.values[1])\n",
    "\n",
    "val3 = load_data(t2023_val.values[2])\n",
    "train3 = load_data(t2023_train.values[2])\n",
    "\n",
    "val4 = load_data(t2023_val.values[3])\n",
    "train4 = load_data(t2023_train.values[3])\n",
    "\n",
    "val5 = load_data(t2023_val.values[5])\n",
    "train5 = load_data(t2023_train.values[5])\n",
    "\n",
    "val6 = load_data(t2023_val.values[5])\n",
    "train6 = load_data(t2023_train.values[5])\n",
    "\n",
    "val7 = load_data(t2023_val.values[6])\n",
    "train7 = load_data(t2023_train.values[6])\n",
    "\n",
    "val8 = load_data(t2023_val.values[7])\n",
    "train8 = load_data(t2023_train.values[7])\n",
    "\n",
    "val9 = load_data(t2023_val.values[8])\n",
    "train9 = load_data(t2023_train.values[8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function twotemp_labels_features, we can create ML-ready features and labels for the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1, labels_1, val_features1, val_labels_1 = twotemp_labels_features(train1, val1)\n",
    "features2, labels_2, val_features2, val_labels_2 = twotemp_labels_features(train2, val2)\n",
    "features3, labels_3, val_features3, val_labels_3 = twotemp_labels_features(train3, val3)\n",
    "features4, labels_4, val_features4, val_labels_4 = twotemp_labels_features(train4, val4)\n",
    "features5, labels_5, val_features5, val_labels_5 = twotemp_labels_features(train5, val5)\n",
    "features6, labels_6, val_features6, val_labels_6 = twotemp_labels_features(train6, val6)\n",
    "features7, labels_7, val_features7, val_labels_7 = twotemp_labels_features(train7, val7)\n",
    "features8, labels_8, val_features8, val_labels_8 = twotemp_labels_features(train8, val8)\n",
    "features9, labels_9, val_features9, val_labels_9 = twotemp_labels_features(train9, val9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and train models\n",
    "\n",
    "Here, we'll use intentionally overfit settings to create an overfit model. This particular instance uses 2 layers containint 20 nodes each. We've increased the batch size to 64 from the previous iteration, and kept the patience at 200 and the drop out at 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.keras.utils.set_random_seed(settings[\"leaky_basic_5\"][\"random_seed\"])\n",
    "\n",
    "# define the early stopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "  monitor=\"val_loss\", \n",
    "  patience=settings[\"leaky_basic_5\"][\"patience\"], \n",
    "  restore_best_weights=True, \n",
    "  mode=\"auto\"\n",
    ")\n",
    "\n",
    "## TS cross 1\n",
    "model_1 = build_model(\n",
    "  features1, \n",
    "  labels_1, \n",
    "  settings[\"leaky_basic_5\"])\n",
    "\n",
    "model_1 = compile_model(\n",
    "  model_1, \n",
    "  settings[\"leaky_basic_5\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_1 = model_1.fit(\n",
    "  features1, \n",
    "  labels_1, \n",
    "  epochs=settings[\"leaky_basic_5\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"leaky_basic_5\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features1, val_labels_1],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## TS cross 2\n",
    "model_2 = build_model(\n",
    "  features2,\n",
    "  labels_2, \n",
    "  settings[\"leaky_basic_5\"])\n",
    "model_2 = compile_model(model_2, settings[\"leaky_basic_5\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_2 = model_2.fit(\n",
    "  features2,\n",
    "  labels_2,\n",
    "  epochs=settings[\"leaky_basic_5\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"leaky_basic_5\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features2, val_labels_2],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## TS cross 3\n",
    "\n",
    "model_3 = build_model(\n",
    "  features3,\n",
    "  labels_3,\n",
    "  settings[\"leaky_basic_5\"])\n",
    "model_3 = compile_model(model_3, settings[\"leaky_basic_5\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_3 = model_3.fit(\n",
    "  features3,\n",
    "  labels_3,\n",
    "  epochs=settings[\"leaky_basic_5\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"leaky_basic_5\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features3, val_labels_3],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## TS cross 4\n",
    "\n",
    "model_4 = build_model(\n",
    "  features4,\n",
    "  labels_4,\n",
    "  settings[\"leaky_basic_5\"])\n",
    "model_4 = compile_model(model_4, settings[\"leaky_basic_5\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_4 = model_4.fit(\n",
    "  features4,\n",
    "  labels_4,\n",
    "  epochs=settings[\"leaky_basic_5\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"leaky_basic_5\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features4, val_labels_4],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## TS cross 5\n",
    "\n",
    "model_5 = build_model(\n",
    "  features5,\n",
    "  labels_5,\n",
    "  settings[\"leaky_basic_5\"])\n",
    "model_5 = compile_model(model_5, settings[\"leaky_basic_5\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_5 = model_5.fit(\n",
    "  features5,\n",
    "  labels_5,\n",
    "  epochs=settings[\"leaky_basic_5\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"leaky_basic_5\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features5, val_labels_5],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## TS cross 6\n",
    "\n",
    "model_6 = build_model(\n",
    "  features6,\n",
    "  labels_6,\n",
    "  settings[\"leaky_basic_5\"])\n",
    "model_6 = compile_model(model_6, settings[\"leaky_basic_5\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_6 = model_6.fit(\n",
    "  features6,\n",
    "  labels_6,\n",
    "  epochs=settings[\"leaky_basic_5\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"leaky_basic_5\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features6, val_labels_6],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## TS cross 7\n",
    "\n",
    "model_7 = build_model(\n",
    "  features7,\n",
    "  labels_7,\n",
    "  settings[\"leaky_basic_5\"])\n",
    "model_7 = compile_model(model_7, settings[\"leaky_basic_5\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_7 = model_7.fit(\n",
    "  features7,\n",
    "  labels_7,\n",
    "  epochs=settings[\"leaky_basic_5\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"leaky_basic_5\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features7, val_labels_7],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## TS cross 8\n",
    "\n",
    "model_8 = build_model(\n",
    "  features8,\n",
    "  labels_8,\n",
    "  settings[\"leaky_basic_5\"])\n",
    "model_8 = compile_model(model_8, settings[\"leaky_basic_5\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_8 = model_8.fit(\n",
    "  features8,\n",
    "  labels_8,\n",
    "  epochs=settings[\"leaky_basic_5\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"leaky_basic_5\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features8, val_labels_8],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n",
    "\n",
    "## TS cross 9\n",
    "\n",
    "model_9 = build_model(\n",
    "  features9,\n",
    "  labels_9,\n",
    "  settings[\"leaky_basic_5\"])\n",
    "model_9 = compile_model(model_9, settings[\"leaky_basic_5\"])\n",
    "\n",
    "# train the model via model.fit\n",
    "history_9 = model_9.fit(\n",
    "  features9,\n",
    "  labels_9,\n",
    "  epochs=settings[\"leaky_basic_5\"][\"max_epochs\"],\n",
    "  batch_size=settings[\"leaky_basic_5\"][\"batch_size\"],\n",
    "  shuffle=True,\n",
    "  validation_data=[val_features9, val_labels_9],\n",
    "  callbacks=[early_stopping_callback],\n",
    "  verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And save the models and training history, to do this, you'll need to create the following directory path:\n",
    "\n",
    "data/NN_train_val_test/SMR_autoNN_reduce_2/models/leaky_basic_5_t2023/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_dir = \"/Users/steeleb/Documents/GitHub/NASA-NW/data/NN_train_val_test/SMR_autoNN_reduce_2/models/leaky_basic_5_t2023/\"\n",
    "\n",
    "# save models to pickle\n",
    "models = [model_1, model_2, model_3, model_4, model_5, model_6, model_7, model_8, model_9]\n",
    "\n",
    "for model, i in zip(models, range(1,10)):\n",
    "    save_to_pickle(model, f\"{dump_dir}/model_{i}.pkl\")\n",
    "\n",
    "# save history to pickles\n",
    "histories = [history_1, history_2, history_3, history_4, history_5, history_6, history_7, history_8, history_9]\n",
    "\n",
    "for history, i in zip(histories, range(1,10)):\n",
    "    save_to_pickle(history, f\"{dump_dir}/history_{i}.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ATSML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
