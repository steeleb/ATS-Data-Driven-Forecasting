{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "This script is an implementation of SHAP (SHapley Additive exPlanations) for the `leaky_basic_5` model. \n",
    "\n",
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shap\n",
    "import imp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "shap.explainers._deep.deep_tf.op_handlers[\"LeakyRelu\"] = shap.explainers._deep.deep_tf.op_handlers[\"Relu\"]\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import custom modules\n",
    "this_dir = \"/Users/steeleb/Documents/GitHub/NASA-NW/modeling/temperature/arNN_fewer2/\"\n",
    "imp.load_source(\"universals\", os.path.join(this_dir, \"universal_functions.py\"))\n",
    "from universals import load_pickle_file, twotemp_labels_features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and models\n",
    "\n",
    "First, we'll load in the training sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/steeleb/Documents/GitHub/NASA-NW/data/NN_train_val_test/SMR_autoNN_reduce_2/\"\n",
    "\n",
    "all_files = pd.Series(os.listdir(data_dir))\n",
    "t2023 = all_files[all_files.str.contains('t2023')]\n",
    "t2023_train = t2023[t2023.str.contains('training')]\n",
    "\n",
    "# these files end up in no particular order, so we need to sort them\n",
    "t2023_train = t2023_train.sort_values()\n",
    "\n",
    "def load_data(file):\n",
    "    return pd.read_csv(os.path.join(data_dir, file), sep=',')\n",
    "\n",
    "train1 = load_data(t2023_train.values[0])\n",
    "\n",
    "train2 = load_data(t2023_train.values[1])\n",
    "\n",
    "train3 = load_data(t2023_train.values[2])\n",
    "\n",
    "train4 = load_data(t2023_train.values[3])\n",
    "\n",
    "train5 = load_data(t2023_train.values[5])\n",
    "\n",
    "train6 = load_data(t2023_train.values[5])\n",
    "\n",
    "train7 = load_data(t2023_train.values[6])\n",
    "\n",
    "train8 = load_data(t2023_train.values[7])\n",
    "\n",
    "train9 = load_data(t2023_train.values[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we need to process these for ML-ready format. Since we're only using the training data, we can use the get_features_labels_test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_1, labels_1 = twotemp_labels_features_test(train1)\n",
    "features_2, labels_2 = twotemp_labels_features_test(train2)\n",
    "features_3, labels_3 = twotemp_labels_features_test(train3)\n",
    "features_4, labels_4 = twotemp_labels_features_test(train4)\n",
    "features_5, labels_5 = twotemp_labels_features_test(train5)\n",
    "features_6, labels_6 = twotemp_labels_features_test(train6)\n",
    "features_7, labels_7 = twotemp_labels_features_test(train7)\n",
    "features_8, labels_8 = twotemp_labels_features_test(train8)\n",
    "features_9, labels_9 = twotemp_labels_features_test(train9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the pickle files for each of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/Users/steeleb/Documents/GitHub/NASA-NW/data/NN_train_val_test/SMR_autoNN_reduce_2/models/leaky_basic_5_t2023/\"\n",
    "\n",
    "models = [f for f in os.listdir(model_dir) if 'history' not in f]\n",
    "\n",
    "model_1 = load_pickle_file(models[0], model_dir)\n",
    "model_2 = load_pickle_file(models[1], model_dir)\n",
    "model_3 = load_pickle_file(models[2], model_dir)\n",
    "model_4 = load_pickle_file(models[3], model_dir)\n",
    "model_5 = load_pickle_file(models[4], model_dir)\n",
    "model_6 = load_pickle_file(models[5], model_dir)\n",
    "model_7 = load_pickle_file(models[6], model_dir)\n",
    "model_8 = load_pickle_file(models[7], model_dir)\n",
    "model_9 = load_pickle_file(models[8], model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_1 = shap.DeepExplainer(model_1, pd.DataFrame.to_numpy(features_1))\n",
    "shap_values_1 = explainer_1.shap_values(pd.DataFrame.to_numpy(features_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_1 = shap.summary_plot(shap_values_1, features_1, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce noise by taking means of sorted 1% segments of the data\n",
    "def get_percentile_stats(arr, sortarr, num=100, func=np.mean):\n",
    "    sorted_arr = arr[np.argsort(sortarr)]  # Sorts arr based on values in sortarr\n",
    "    split_arrs = np.array_split(sorted_arr, num)  # Splits the array into [num] lists\n",
    "    meanslist = list(map(func, split_arrs))  # Applies np.mean to all arrays in split_arrs\n",
    "    return np.array(meanslist)  # Returns array of the means for [num] sorted segments of arr\n",
    "\n",
    "\n",
    "# Plot out the Shapley Values in a more visually appealing format\n",
    "def plot_shap(shapvals, featurevals, feature_names, mod_identity):\n",
    "    # Number of samples\n",
    "    samp_num = shapvals.shape[0]\n",
    "\n",
    "    # Init colormap\n",
    "    n = len(feature_names)\n",
    "    color = iter(plt.cm.get_cmap(\"viridis\")(np.linspace(0, 1, n)))\n",
    "\n",
    "    for varindex, varname in enumerate(feature_names):\n",
    "        # Step color\n",
    "        c = next(color)\n",
    "\n",
    "        # Get the avg feature val for every 5 percentiles of shap values\n",
    "        featuremean_for_shappercentile = get_percentile_stats(featurevals[:, varindex], shapvals[:, varindex])\n",
    "\n",
    "        # Get the median shap val for every 5 percentiles of shap values\n",
    "        shapmedian_for_shappercentile = get_percentile_stats(\n",
    "            shapvals[:, varindex], shapvals[:, varindex], func=np.median\n",
    "        )\n",
    "\n",
    "        # Plot\n",
    "        plt.plot(featuremean_for_shappercentile, shapmedian_for_shappercentile, \"o\", label=varname, color=c)\n",
    "        plt.axhline(0, zorder=0, color=\"k\", alpha=0.1)\n",
    "        plt.axvline(0, zorder=0, color=\"k\", alpha=0.1)\n",
    "\n",
    "    plt.xlim(-3, 3)\n",
    "    plt.legend(bbox_to_anchor=(1.6, 1), loc=\"upper right\")\n",
    "\n",
    "    plt.ylabel(\"Median Shap Value\")\n",
    "    plt.xlabel(\"Mean Feature Value\")\n",
    "    plt.title(\n",
    "        \"SHAP Values and Feature Values For Model \" + str(mod_identity)\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap(shap_values_1[0], pd.DataFrame.to_numpy(features_1), features_1.columns, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_1[0], features_1)\n",
    "shap.summary_plot(shap_values_1[1], features_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No one is surprised here, the most important features are yesterday's temperature values. While pumping is not 'highly important', it is one of the few features that has a strong negative impact on the model temperature prediction (i.e. the higher the pumping, the lower the temperature). Which is what we want to see. Given this, let's apply this model to the test data, see how it performs, and then play with the pump values to see how it impacts the temperature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_NW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
